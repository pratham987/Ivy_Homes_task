v1_extractor.py

This Python script is an asynchronous web scraper designed to extract autocomplete suggestions from the v1 version of the API while adhering to rate limits. The script leverages the aiohttp library for making non-blocking HTTP requests and uses a priority queue (min-heap) to efficiently manage the prefixes being queried. It starts with the letter "a" and progressively expands its search space by appending new characters based on API responses. The fetch_autocomplete_results function ensures that API rate limits are respected by introducing a delay between requests, while a semaphore controls concurrent access to avoid overwhelming the server. The core logic involves maintaining a set of discovered words to prevent redundant queries and iteratively expanding the search space by pushing new prefixes derived from retrieved suggestions. The next_lexicographic_string function helps generate the next possible prefix in lexicographical order, ensuring a systematic exploration of potential autocomplete results. As results are retrieved, they are written to a file in real-time to prevent data loss in case of interruptions. The script efficiently prioritizes exploration of shorter prefixes first, ensuring an exhaustive yet optimized traversal of the autocomplete space. Overall, this approach balances efficiency with compliance to API constraints, making it a structured and scalable solution for collecting autocomplete data.

v2_extractor.py

This Python script is an asynchronous web scraper designed to extract alphanumeric autocomplete suggestions from the v2 version of the API while adhering to rate limits. The primary change from the previous version is the update to v2 of the API, which now supports both letters and numbers. The script uses aiohttp for non-blocking HTTP requests and a priority queue (min-heap) to manage queried prefixes efficiently. Initially, it populates the queue with all digits (0-9) and lowercase letters (a-z), ensuring a broader search space. The fetch_autocomplete_results function manages API calls with a semaphore to limit concurrent requests and introduces a delay to comply with the API's rate limits. The function next_lexicographic_string has been modified to handle alphanumeric sequences by progressing through numbers and letters systematically, ensuring a structured exploration of possible completions. As results are retrieved, they are written to a file in real-time to prevent data loss, and discovered words are tracked to avoid redundant queries. The script prioritizes expanding shorter prefixes first, ensuring an exhaustive yet efficient traversal of the autocomplete space. These enhancements make the scraper more comprehensive, allowing it to collect a wider range of suggestions while maintaining efficiency and compliance with API constraints.

v3_extractor.py

This python script is designed to efficiently scrape autocomplete suggestions from an API while adhering to rate limits. The core idea behind the implementation is to use a priority queue (min-heap) to systematically explore prefixes and discover all possible words suggested by the API. The process starts by seeding the queue with single-character and two-character prefixes composed of alphanumeric characters and special symbols. Using an asynchronous approach with aiohttp, the script sends API requests to fetch autocomplete results while ensuring compliance with the API's rate limit of 75 requests per minute. A semaphore limits concurrent requests to ten at a time to optimize performance. The script maintains a set of discovered words to avoid duplication and writes them into a file for persistent storage. To ensure that no valid words are missed, it iteratively expands prefixes based on the returned suggestions and also calculates the next lexicographic string to explore potential words beyond the current results. The program dynamically adjusts the search space by progressively extending the queried prefixes, thus allowing it to uncover as many words as possible without making redundant API calls. By leveraging asynchronous processing, priority queue-based prefix expansion, and intelligent rate limiting, this scraper efficiently extracts a comprehensive list of autocomplete suggestions while minimizing unnecessary requests.

limit.py
This script is designed to test the rate limits of an autocomplete API by sending a large number of requests within a defined time period. The core idea is to repeatedly query the API endpoint while ensuring that requests are spaced out to comply with the given time constraint. The script defines a total request count of 500 and a duration of 300 seconds (5 minutes), meaning the requests are ideally spaced to maintain a consistent rate. Each request is sent using the requests library, and the status code of the response is logged to monitor the API’s behavior. If the API responds with a 429 status code (Too Many Requests), indicating that the rate limit has been exceeded, the script stops execution and reports the number of successful requests made before hitting the limit. Otherwise, it calculates the time elapsed and adjusts the sleep duration between requests to evenly distribute them within the specified time frame. The approach ensures a systematic load test of the API’s rate limit while avoiding unnecessary excessive requests.
